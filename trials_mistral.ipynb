{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = Llama(model_path=\"../llama.cpp/models/Mistral-7B-v0.1/mistral-7b-v0.1.q5-k-m.gguf\", n_threads=4, verbose=True)\n",
    "llm = Llama(model_path=\"models/mistral-7b-instruct-v0.1.gguf\", n_threads=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[], echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Run a command in the terminal\n",
    "os.system(f\"say {'Hey, I am a llama. I am a language model that can generate text.'}\")\n",
    "\n",
    "vocal_output = \"What can I do for you?\"\n",
    "\n",
    "os.system(f\"say {vocal_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "We want to talk to this AI, right? We don't want, however, to have to wait for a whopping 30sec every time we need to say something.\n",
    "The first thing is to know when we have some kind of interaction expected by the user. We could potentially only feed the transcribed text to Mistral, or hopefully a much lighter model, to see if it's a question or a command.\n",
    "If it is, then we feed it to a larger model that is going to think. And then, if an action is required, we're going to transfer to a model that is going to do the action.\n",
    "\n",
    "Okay so, here's what we can do. Every 5 seconds, we run the fastest whisper wrapper we have, over the past 20-25 seconds that have been asked by the user, and then we make the text go through an small LLM to see if it's a question or a command. Say... Mistral quantized? Or what other, smaller model could we use? Let's check the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"models/mistral-7b-instruct-v0.1.gguf\", n_threads=6, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_instruction(text):\n",
    "    llm_input = \"[INST]You are an AI that's designed to recognize when a user is calling you for assistance. If they don't ask you, it's NOT a request or instruction. \" \\\n",
    "        \"Determine if the following transcript contains a direct instruction or request for your help:\\n\\n\" \\\n",
    "        f\"'{text}'\" \\\n",
    "        \"\\n\\nReply with 'True' if there's a direct instruction or request, and 'False' if not. Please be exact in your response.[/INST]\"\n",
    "        \n",
    "    # print(llm_input)\n",
    "    output = llm(llm_input)\n",
    "    \n",
    "    return True if \"True\" in output[\"choices\"][0][\"text\"] else (False if \"False\" in output[\"choices\"][0][\"text\"] else None)\n",
    "\n",
    "is_instruction(\"Can you show me my latest emails?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_instruction_2(text):\n",
    "    llm_input = \"[INST]You are Jarvis, an AI that's designed to recognize when a user is calling you for assistance. A call for assistance must include an explicit call to you, your name is 'Jarvis' \" \\\n",
    "        \"Determine if the following transcript contains a direct instruction or request for your help:\\n\\n\" \\\n",
    "        f\"'{text}'\" \\\n",
    "        \"\\n\\nReply with 'True' if there's a direct instruction or request, and 'False' if not. Please be exact in your response.[/INST]\"\n",
    "        \n",
    "    # print(llm_input)\n",
    "    output = llm(llm_input)\n",
    "    \n",
    "    return True if \"True\" in output[\"choices\"][0][\"text\"] else (False if \"False\" in output[\"choices\"][0][\"text\"] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_instruction_2(\"Hello Jarvis, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_instruction(\"I like trains.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_instruction(\"Yeah so I was on the phone with Phillis and\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay good so, so far, Mistral-Instruct-Q5_K_M seems to work well for this! Step 1 completed. Now, we need some speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "\n",
    "# Add the CT2_VERBOSE=1 flag to the environment variables\n",
    "os.environ[\"CT2_VERBOSE\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "model_size = \"medium.en\"\n",
    "\n",
    "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, info = model.transcribe(\"mp3s/1.mp3\")\n",
    "transcribed_segments = list(segments)\n",
    "# 10sec for 1.mp3 in float32\n",
    "# 6sec in int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, info = model.transcribe(\"mp3s/2.mp3\")\n",
    "transcribed_segments = list(segments)\n",
    "# 10sec for 2.mp3\n",
    "# 6.7sec in int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_text = \" \".join([segment.text for segment in transcribed_segments])\n",
    "transcribed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_instruction_2(transcribed_text)\n",
    "# 19sec for 2.mp3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work pretty fine! Now we need to automate the audio collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def record_audio(filename, duration, samplerate=44100):\n",
    "    print(\"Recording...\")\n",
    "    myrecording = sd.rec(int(samplerate * duration), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording finished.\")\n",
    "    \n",
    "    # Convert recording to AudioSegment for easy export\n",
    "    \n",
    "    print(myrecording.dtype.itemsize)\n",
    "    \n",
    "    audio = AudioSegment(\n",
    "        myrecording.tobytes(),\n",
    "        frame_rate=samplerate,\n",
    "        sample_width=myrecording.dtype.itemsize,\n",
    "        channels=1\n",
    "    )\n",
    "    \n",
    "    audio.export(filename, format=\"mp3\", bitrate=\"128k\")\n",
    "    print(f\"File saved as {filename}\")\n",
    "\n",
    "# Usage example:\n",
    "record_audio(\"output.mp3\", 2)  # Records for 10 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame.mixer\n",
    "import time\n",
    "\n",
    "def play_audio(filename):\n",
    "    # Initialize the mixer module\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    \n",
    "    print(f\"Playing {filename}...\")\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # This will keep the program running while the audio plays\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(\"Playback finished.\")\n",
    "\n",
    "# Usage example:\n",
    "play_audio(\"output.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "\n",
    "# Add the CT2_VERBOSE=1 flag to the environment variables\n",
    "os.environ[\"CT2_VERBOSE\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "model_size = \"medium.en\"\n",
    "\n",
    "whisper = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "segments, info = model.transcribe(\"output.mp3\")\n",
    "transcribed_segments = list(segments)\n",
    "transcribed_text = \" \".join([segment.text for segment in transcribed_segments])\n",
    "transcribed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, cool! Now all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-11 14:12:55.576] [ctranslate2] [thread 1096624] [info] Loaded model /Users/axelpeytavin/.cache/huggingface/hub/models--guillaumekln--faster-whisper-medium.en/snapshots/83a3b718775154682e5f775bc5d5fc961d2350ce on device cpu:0\n",
      "[2023-10-11 14:12:55.576] [ctranslate2] [thread 1096624] [info]  - Binary version: 6\n",
      "[2023-10-11 14:12:55.576] [ctranslate2] [thread 1096624] [info]  - Model specification revision: 3\n",
      "[2023-10-11 14:12:55.576] [ctranslate2] [thread 1096624] [info]  - Selected compute type: int8_float32\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import pygame.mixer\n",
    "import time\n",
    "\n",
    "\n",
    "# Add the CT2_VERBOSE=1 flag to the environment variables\n",
    "os.environ[\"CT2_VERBOSE\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "model_size = \"medium.en\"\n",
    "whisper = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "def record_audio(filename, duration, samplerate=44100):\n",
    "    print(\"Recording...\")\n",
    "    myrecording = sd.rec(int(samplerate * duration), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording finished.\")\n",
    "    \n",
    "    # Convert recording to AudioSegment for easy export\n",
    "    \n",
    "    print(myrecording.dtype.itemsize)\n",
    "    \n",
    "    audio = AudioSegment(\n",
    "        myrecording.tobytes(),\n",
    "        frame_rate=samplerate,\n",
    "        sample_width=myrecording.dtype.itemsize,\n",
    "        channels=1\n",
    "    )\n",
    "    \n",
    "    audio.export(filename, format=\"mp3\", bitrate=\"128k\")\n",
    "    print(f\"File saved as {filename}\")\n",
    "\n",
    "\n",
    "def play_audio(filename):\n",
    "    # Initialize the mixer module\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    \n",
    "    print(f\"Playing {filename}...\")\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # This will keep the program running while the audio plays\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(\"Playback finished.\")\n",
    "\n",
    "\n",
    "def transcribe_recording(filename):\n",
    "    segments, info = model.transcribe(filename)\n",
    "    transcribed_segments = list(segments)\n",
    "    transcribed_text = \" \".join([segment.text for segment in transcribed_segments])\n",
    "    return transcribed_text\n",
    "\n",
    "def record_and_transcribe(filename, duration):\n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "    record_audio(filename, duration)\n",
    "    print(\"Recording finished. Now transcribing...\")\n",
    "    return transcribe_recording(filename)\n",
    "\n",
    "def query_llm(text):\n",
    "    llm_input = f\"[INST]You are my AI partner Jarvis. Here is my prompt for you: {text}[/INST]\"\n",
    "        \n",
    "    # print(llm_input)\n",
    "    output = llm(llm_input)\n",
    "    \n",
    "    return output[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def talk_to_llm(duration=3):\n",
    "    text = record_and_transcribe(\"temp.mp3\", duration)\n",
    "\n",
    "    print(\"You said:\", text)\n",
    "    response = query_llm(text)\n",
    "    \n",
    "    response = response.replace('\\'', '\"')\n",
    "\n",
    "    os.system(f\"say '{response}'\")\n",
    "\n",
    "# Total length: 46sec for \"Hey Jarvis how's it going?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for 10 seconds...\n",
      "Recording...\n",
      "Recording finished.\n",
      "2\n",
      "File saved as temp.mp3\n",
      "Recording finished. Now transcribing...\n",
      "You said:  okay Jarvis I'm trying to give you access to everything you can do on my computer  and say I do that what are the kind of tasks that you can do\n"
     ]
    }
   ],
   "source": [
    "talk_to_llm(10)\n",
    "\n",
    "# This is WILD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/ng/64vj_sh13fn7kr98h1jq07h40000gn/T/ipykernel_29402/2253239688.py\", line 6, in process_audio\n",
      "  File \"/var/folders/ng/64vj_sh13fn7kr98h1jq07h40000gn/T/ipykernel_29402/1500008034.py\", line 53, in transcribe_recording\n",
      "  File \"/Users/axelpeytavin/Documents/Projects/self-improving-ai/venv/lib/python3.9/site-packages/faster_whisper/transcribe.py\", line 258, in transcribe\n",
      "    audio = decode_audio(audio, sampling_rate=sampling_rate)\n",
      "  File \"/Users/axelpeytavin/Documents/Projects/self-improving-ai/venv/lib/python3.9/site-packages/faster_whisper/audio.py\", line 46, in decode_audio\n",
      "    with av.open(input_file, metadata_errors=\"ignore\") as container:\n",
      "  File \"av/container/core.pyx\", line 401, in av.container.core.open\n",
      "  File \"av/container/core.pyx\", line 246, in av.container.core.Container.__cinit__\n",
      "  File \"av/container/pyio.pyx\", line 44, in av.container.pyio.PyIOFile.__cinit__\n",
      "ValueError: File object has no read() method, or readable() returned False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording finished.\n",
      "2\n",
      "File saved as 2023-10-11 14:36:08.mp3\n",
      "Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/ng/64vj_sh13fn7kr98h1jq07h40000gn/T/ipykernel_29402/2253239688.py\", line 6, in process_audio\n",
      "  File \"/var/folders/ng/64vj_sh13fn7kr98h1jq07h40000gn/T/ipykernel_29402/1500008034.py\", line 53, in transcribe_recording\n",
      "  File \"/Users/axelpeytavin/Documents/Projects/self-improving-ai/venv/lib/python3.9/site-packages/faster_whisper/transcribe.py\", line 258, in transcribe\n",
      "    audio = decode_audio(audio, sampling_rate=sampling_rate)\n",
      "  File \"/Users/axelpeytavin/Documents/Projects/self-improving-ai/venv/lib/python3.9/site-packages/faster_whisper/audio.py\", line 46, in decode_audio\n",
      "    with av.open(input_file, metadata_errors=\"ignore\") as container:\n",
      "  File \"av/container/core.pyx\", line 401, in av.container.core.open\n",
      "  File \"av/container/core.pyx\", line 246, in av.container.core.Container.__cinit__\n",
      "  File \"av/container/pyio.pyx\", line 44, in av.container.pyio.PyIOFile.__cinit__\n",
      "ValueError: File object has no read() method, or readable() returned False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording finished.\n",
      "2\n",
      "File saved as 2023-10-11 14:36:12.mp3\n",
      "Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/ng/64vj_sh13fn7kr98h1jq07h40000gn/T/ipykernel_29402/2253239688.py\", line 6, in process_audio\n",
      "  File \"/var/folders/ng/64vj_sh13fn7kr98h1jq07h40000gn/T/ipykernel_29402/1500008034.py\", line 53, in transcribe_recording\n",
      "  File \"/Users/axelpeytavin/Documents/Projects/self-improving-ai/venv/lib/python3.9/site-packages/faster_whisper/transcribe.py\", line 258, in transcribe\n",
      "    audio = decode_audio(audio, sampling_rate=sampling_rate)\n",
      "  File \"/Users/axelpeytavin/Documents/Projects/self-improving-ai/venv/lib/python3.9/site-packages/faster_whisper/audio.py\", line 46, in decode_audio\n",
      "    with av.open(input_file, metadata_errors=\"ignore\") as container:\n",
      "  File \"av/container/core.pyx\", line 401, in av.container.core.open\n",
      "  File \"av/container/core.pyx\", line 246, in av.container.core.Container.__cinit__\n",
      "  File \"av/container/pyio.pyx\", line 44, in av.container.pyio.PyIOFile.__cinit__\n",
      "ValueError: File object has no read() method, or readable() returned False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording finished.\n",
      "2\n",
      "File saved as 2023-10-11 14:36:15.mp3\n",
      "Recording...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         threading\u001b[39m.\u001b[39mThread(target\u001b[39m=\u001b[39mprocess_audio, args\u001b[39m=\u001b[39m(record_file,))\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39m# Sleep for a short period to allow the next recording to start shortly after the previous one finishes.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39m# This helps to make sure there's no noticeable gap between recordings.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39m# time.sleep(duration + 0.1)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m talk_to_llm()\n",
      "\u001b[1;32m/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Give it a unique filename so we don't overwrite the previous recording. Say Date - Time in H:M\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     f_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.mp3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     record_file \u001b[39m=\u001b[39m record_audio(f_name, duration)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     threading\u001b[39m.\u001b[39mThread(target\u001b[39m=\u001b[39mprocess_audio, args\u001b[39m=\u001b[39m(record_file,))\u001b[39m.\u001b[39mstart()\n",
      "\u001b[1;32m/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRecording...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m myrecording \u001b[39m=\u001b[39m sd\u001b[39m.\u001b[39mrec(\u001b[39mint\u001b[39m(samplerate \u001b[39m*\u001b[39m duration), samplerate\u001b[39m=\u001b[39msamplerate, channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mint16\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m sd\u001b[39m.\u001b[39;49mwait()  \u001b[39m# Wait until recording is finished\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRecording finished.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/axelpeytavin/Documents/Projects/self-improving-ai/trials_mistral.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Convert recording to AudioSegment for easy export\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/self-improving-ai/venv/lib/python3.9/site-packages/sounddevice.py:395\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ignore_errors)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[39mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m _last_callback:\n\u001b[0;32m--> 395\u001b[0m     \u001b[39mreturn\u001b[39;00m _last_callback\u001b[39m.\u001b[39;49mwait(ignore_errors)\n",
      "File \u001b[0;32m~/Documents/Projects/self-improving-ai/venv/lib/python3.9/site-packages/sounddevice.py:2601\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \n\u001b[1;32m   2597\u001b[0m \u001b[39mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m \n\u001b[1;32m   2599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2601\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m   2602\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def process_audio(filename):\n",
    "    transcription = transcribe_recording(filename)\n",
    "    print(\"You said:\", transcription)\n",
    "    \n",
    "    if is_instruction(transcription):\n",
    "        response = query_llm(transcription)\n",
    "        response = response.replace('\\'', '\"')\n",
    "        os.system(f\"say '{response}'\")\n",
    "\n",
    "def talk_to_llm(duration=3):\n",
    "    while True:\n",
    "        # Give it a unique filename so we don't overwrite the previous recording. Say Date - Time in H:M\n",
    "        f_name = f\"{time.strftime('%Y-%m-%d %H:%M:%S')}.mp3\"\n",
    "        record_file = record_audio(f_name, duration)\n",
    "        threading.Thread(target=process_audio, args=(record_file,)).start()\n",
    "        \n",
    "        # Sleep for a short period to allow the next recording to start shortly after the previous one finishes.\n",
    "        # This helps to make sure there's no noticeable gap between recordings.\n",
    "        # time.sleep(duration + 0.1)\n",
    "\n",
    "talk_to_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
