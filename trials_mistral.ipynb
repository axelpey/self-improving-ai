{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = Llama(model_path=\"../llama.cpp/models/Mistral-7B-v0.1/mistral-7b-v0.1.q5-k-m.gguf\", n_threads=4, verbose=True)\n",
    "llm = Llama(model_path=\"models/mistral-7b-instruct-v0.1.gguf\", n_threads=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[], echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Run a command in the terminal\n",
    "os.system(f\"say {'Hey, I am a llama. I am a language model that can generate text.'}\")\n",
    "\n",
    "vocal_output = \"What can I do for you?\"\n",
    "\n",
    "os.system(f\"say {vocal_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "We want to talk to this AI, right? We don't want, however, to have to wait for a whopping 30sec every time we need to say something.\n",
    "The first thing is to know when we have some kind of interaction expected by the user. We could potentially only feed the transcribed text to Mistral, or hopefully a much lighter model, to see if it's a question or a command.\n",
    "If it is, then we feed it to a larger model that is going to think. And then, if an action is required, we're going to transfer to a model that is going to do the action.\n",
    "\n",
    "Okay so, here's what we can do. Every 5 seconds, we run the fastest whisper wrapper we have, over the past 20-25 seconds that have been asked by the user, and then we make the text go through an small LLM to see if it's a question or a command. Say... Mistral quantized? Or what other, smaller model could we use? Let's check the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"models/mistral-7b-instruct-v0.1.gguf\", n_threads=6, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_instruction(text):\n",
    "    llm_input = \"[INST]You are an AI that's designed to recognize when a user is calling you for assistance. If they don't ask you, it's NOT a request or instruction. \" \\\n",
    "        \"Determine if the following transcript contains a direct instruction or request for your help:\\n\\n\" \\\n",
    "        f\"'{text}'\" \\\n",
    "        \"\\n\\nReply with 'True' if there's a direct instruction or request, and 'False' if not. Please be exact in your response.[/INST]\"\n",
    "        \n",
    "    # print(llm_input)\n",
    "    output = llm(llm_input)\n",
    "    \n",
    "    return True if \"True\" in output[\"choices\"][0][\"text\"] else (False if \"False\" in output[\"choices\"][0][\"text\"] else None)\n",
    "\n",
    "is_instruction(\"Can you show me my latest emails?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_instruction_2(text):\n",
    "    llm_input = \"[INST]You are Jarvis, an AI that's designed to recognize when a user is calling you for assistance. A call for assistance must include an explicit call to you, your name is 'Jarvis' \" \\\n",
    "        \"Determine if the following transcript contains a direct instruction or request for your help:\\n\\n\" \\\n",
    "        f\"'{text}'\" \\\n",
    "        \"\\n\\nReply with 'True' if there's a direct instruction or request, and 'False' if not. Please be exact in your response.[/INST]\"\n",
    "        \n",
    "    # print(llm_input)\n",
    "    output = llm(llm_input)\n",
    "    \n",
    "    return True if \"True\" in output[\"choices\"][0][\"text\"] else (False if \"False\" in output[\"choices\"][0][\"text\"] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_instruction_2(\"Hello Jarvis, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_instruction(\"I like trains.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_instruction(\"Yeah so I was on the phone with Phillis and\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay good so, so far, Mistral-Instruct-Q5_K_M seems to work well for this! Step 1 completed. Now, we need some speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "\n",
    "# Add the CT2_VERBOSE=1 flag to the environment variables\n",
    "os.environ[\"CT2_VERBOSE\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "model_size = \"medium.en\"\n",
    "\n",
    "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, info = model.transcribe(\"mp3s/1.mp3\")\n",
    "transcribed_segments = list(segments)\n",
    "# 10sec for 1.mp3 in float32\n",
    "# 6sec in int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, info = model.transcribe(\"mp3s/2.mp3\")\n",
    "transcribed_segments = list(segments)\n",
    "# 10sec for 2.mp3\n",
    "# 6.7sec in int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_text = \" \".join([segment.text for segment in transcribed_segments])\n",
    "transcribed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_instruction_2(transcribed_text)\n",
    "# 19sec for 2.mp3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work pretty fine! Now we need to automate the audio collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def record_audio(filename, duration, samplerate=44100):\n",
    "    print(\"Recording...\")\n",
    "    myrecording = sd.rec(int(samplerate * duration), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording finished.\")\n",
    "    \n",
    "    # Convert recording to AudioSegment for easy export\n",
    "    \n",
    "    print(myrecording.dtype.itemsize)\n",
    "    \n",
    "    audio = AudioSegment(\n",
    "        myrecording.tobytes(),\n",
    "        frame_rate=samplerate,\n",
    "        sample_width=myrecording.dtype.itemsize,\n",
    "        channels=1\n",
    "    )\n",
    "    \n",
    "    audio.export(filename, format=\"mp3\", bitrate=\"128k\")\n",
    "    print(f\"File saved as {filename}\")\n",
    "\n",
    "# Usage example:\n",
    "record_audio(\"output.mp3\", 2)  # Records for 10 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame.mixer\n",
    "import time\n",
    "\n",
    "def play_audio(filename):\n",
    "    # Initialize the mixer module\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    \n",
    "    print(f\"Playing {filename}...\")\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # This will keep the program running while the audio plays\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(\"Playback finished.\")\n",
    "\n",
    "# Usage example:\n",
    "play_audio(\"output.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "\n",
    "# Add the CT2_VERBOSE=1 flag to the environment variables\n",
    "os.environ[\"CT2_VERBOSE\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "model_size = \"medium.en\"\n",
    "\n",
    "whisper = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "segments, info = model.transcribe(\"output.mp3\")\n",
    "transcribed_segments = list(segments)\n",
    "transcribed_text = \" \".join([segment.text for segment in transcribed_segments])\n",
    "transcribed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, cool! Now all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-11 14:00:58.465] [ctranslate2] [thread 1096624] [info] Loaded model /Users/axelpeytavin/.cache/huggingface/hub/models--guillaumekln--faster-whisper-medium.en/snapshots/83a3b718775154682e5f775bc5d5fc961d2350ce on device cpu:0\n",
      "[2023-10-11 14:00:58.465] [ctranslate2] [thread 1096624] [info]  - Binary version: 6\n",
      "[2023-10-11 14:00:58.465] [ctranslate2] [thread 1096624] [info]  - Model specification revision: 3\n",
      "[2023-10-11 14:00:58.465] [ctranslate2] [thread 1096624] [info]  - Selected compute type: int8_float32\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import pygame.mixer\n",
    "import time\n",
    "\n",
    "\n",
    "# Add the CT2_VERBOSE=1 flag to the environment variables\n",
    "os.environ[\"CT2_VERBOSE\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "model_size = \"medium.en\"\n",
    "whisper = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "def record_audio(filename, duration, samplerate=44100):\n",
    "    print(\"Recording...\")\n",
    "    myrecording = sd.rec(int(samplerate * duration), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording finished.\")\n",
    "    \n",
    "    # Convert recording to AudioSegment for easy export\n",
    "    \n",
    "    print(myrecording.dtype.itemsize)\n",
    "    \n",
    "    audio = AudioSegment(\n",
    "        myrecording.tobytes(),\n",
    "        frame_rate=samplerate,\n",
    "        sample_width=myrecording.dtype.itemsize,\n",
    "        channels=1\n",
    "    )\n",
    "    \n",
    "    audio.export(filename, format=\"mp3\", bitrate=\"128k\")\n",
    "    print(f\"File saved as {filename}\")\n",
    "\n",
    "\n",
    "def play_audio(filename):\n",
    "    # Initialize the mixer module\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    \n",
    "    print(f\"Playing {filename}...\")\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # This will keep the program running while the audio plays\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(\"Playback finished.\")\n",
    "\n",
    "\n",
    "def transcribe_recording(filename):\n",
    "    segments, info = model.transcribe(filename)\n",
    "    transcribed_segments = list(segments)\n",
    "    transcribed_text = \" \".join([segment.text for segment in transcribed_segments])\n",
    "    return transcribed_text\n",
    "\n",
    "def record_and_transcribe(filename, duration):\n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "    record_audio(filename, duration)\n",
    "    print(\"Recording finished. Now transcribing...\")\n",
    "    return transcribe_recording(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for 3 seconds...\n",
      "Recording...\n",
      "Recording finished.\n",
      "2\n",
      "File saved as temp.mp3\n",
      "Recording finished. Now transcribing...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hey, how is it going?'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_and_transcribe(\"temp.mp3\", 3)\n",
    "\n",
    "# 9.7sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
